<div align="center">

<h1>E-RECAP: Embodied REplanning with Cost-Aware Pruning</h1>

</div>

![](https://img.shields.io/github/last-commit/NEBULIS-Lab/E-RECAP?color=green) ![](https://img.shields.io/badge/version-2.0-blue) <a href="https://nebulis-lab.com/"><img src="https://img.shields.io/badge/NEBULIS%20Lab-Website-6366F1.svg?style=for-the-badge&logoColor=white" alt="NEBULIS Lab"></a> <a href="#"><img src="https://img.shields.io/badge/arXiv-coming%20soon-009688.svg" alt="arXiv"></a> <a href="https://pytorch.org/get-started/locally/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white"></a> <a href="https://huggingface.co/docs/transformers"><img alt="Transformers" src="https://img.shields.io/badge/Transformers-ffd21e?logo=huggingface&logoColor=black"></a> <a href="https://www.deepspeed.ai/"><img alt="DeepSpeed" src="https://img.shields.io/badge/DeepSpeed-0A66FF?logo=microsoft&logoColor=white"></a> <a href="https://huggingface.co/docs/accelerate"><img alt="Accelerate" src="https://img.shields.io/badge/Accelerate-ffd21e?logo=huggingface&logoColor=black"></a>

This project implements E-RECAP (Embodied REplanning with Cost-Aware Pruning), a system-level, drop-in method for accelerating replanning in embodied agents by cost-aware pruning of planner context. E-RECAP operates as a Planner optimization module that can be seamlessly integrated into embodied AI systems without modifying task definitions, environments, or control policies.

## Project Structure

```
E-RECAP/
├── checkpoints/          # Model weights and checkpoints
│   ├── pruning_module.pt    # Stage 2 trained Token Pruner (required for inference)
│   ├── saliency.pt          # Stage 1 saliency baseline (optional)
│   └── <model-name>/        # Your model directory (e.g., qwen2-7b-instruct, llama2-7b, etc.)
│       ├── config.json          # Model configuration (required)
│       ├── model.safetensors    # Model weights (or model.bin, required)
│       ├── tokenizer.json       # Tokenizer configuration (required)
│       └── ...                  # Other model files (e.g., generation_config.json, etc.)
│
├── data/                 # Datasets
│   └── raw/                 # Raw data files (e.g., Dolly-15k)
│
├── results/              # Experimental results and reports
│   ├── fig/                 # Visualization figures
│   └── part1_sum.md         # Stage 1 summary report
│
├── scripts/              # Execution scripts
│   ├── run_stage1.sh        # Stage 1: Saliency computation
│   ├── run_stage2.sh        # Stage 2: Pruning module training
│   ├── run_inference.sh     # Single GPU inference
│   ├── run_inference_multigpu.sh  # Multi-GPU inference
│   ├── check_full_env.sh    # Environment check
│   └── install.sh           # Dependency installation
│
└── src/                  # Source code
    ├── stage1_saliency.py        # Stage 1: Gradient × hidden states
    ├── stage2_pruning.py         # Stage 2: Learnable Token Pruner
    ├── erecap_model.py            # Core model with pruning logic
    ├── inference_erecap.py        # Single GPU inference
    ├── inference_erecap_multigpu.py  # Multi-GPU inference
    └── multigpu_test.py          # Multi-GPU memory profiling
```

## Quick Start

### Requirements

- Python 3.10+
- CUDA 12.1+
- **Hardware**: 8× NVIDIA RTX 5880 Ada Generation (48GB VRAM each)
  - Single GPU mode: Uses one GPU
  - Multi-GPU mode: Uses all 8 GPUs
- ≥50GB disk space for model storage

### Installation

**Prerequisites:**
- Install CUDA 12.1+ (includes nvcc compiler) and NVIDIA GPU drivers
- Verify CUDA installation: `nvcc --version` and `nvidia-smi`

**Install Python packages:**
```bash
pip install -r requirements.txt
```

**Note:** PyTorch will automatically use the installed CUDA version. For CUDA 12.x, install PyTorch with:
```bash
pip install torch --index-url https://download.pytorch.org/whl/cu121
```

### File Organization

**Required files and their locations:**

1. **Model files** → `checkpoints/<model-name>/`
   - Place your HuggingFace-compatible model here
   - Must include: `config.json`, model weights (`.safetensors` or `.bin`), tokenizer files
   - Example structure:
     ```
     checkpoints/
     └── your-model-name/
         ├── config.json
         ├── model.safetensors (or model-*.safetensors)
         ├── tokenizer.json
         └── ...
     ```

2. **Pruning module** → `checkpoints/pruning_module.pt`
   - Generated by Stage 2 training
   - Model-specific (tied to model's `hidden_size`)
   - Required for inference

3. **Saliency baseline** → `checkpoints/saliency.pt`
   - Generated by Stage 1 (optional)
   - Used for training pruning module in Stage 2

4. **Training data** → `data/raw/dolly15k/` or `dolly15k/`
   - Used for Stage 1 and Stage 2 training
   - Can use any HuggingFace-compatible dataset

5. **Results** → `results/`
   - All benchmark results and logs are saved here

### Usage

#### Model Setup

**Note:** This repository provides Qwen2-7B-Instruct as an example for running and testing E-RECAP. The required files (`checkpoints/qwen2-7b-instruct/`, `checkpoints/pruning_module.pt`, and `checkpoints/saliency.pt`) are included. However, E-RECAP supports any HuggingFace-compatible Transformer model - the pruning module is model-agnostic and works with any model architecture that has a `hidden_size` configuration.

**Place your model files:**
1. Download or copy your model to `checkpoints/<model-name>/` directory
   - The model directory should contain `config.json`, model weights (`.safetensors` or `.bin`), and tokenizer files
   - Example: `checkpoints/qwen2-7b-instruct/`, `checkpoints/llama2-7b/`, etc.

2. **Configure model path** in `src/inference_erecap.py`:
   ```python
   MODEL_PATH = "checkpoints/<your-model-name>"
   ```
   Replace `<your-model-name>` with your actual model directory name.

#### Pre-flight Check

Verify that model and checkpoints exist (run from project root):
```bash
# Check model (replace <model-name> with your actual model directory)
ls -lh checkpoints/<model-name>/config.json

# Check checkpoints
ls -lh checkpoints/pruning_module.pt checkpoints/saliency.pt
```

If model or checkpoints are missing:
- **Model**: Download/copy your model to `checkpoints/<model-name>/`
- **Pruning module**: Run Stage 2 to train (see below)
- **Saliency**: Run Stage 1 to generate (optional)

#### Quick Verification

Test that all components are ready:
```bash
python3 -c "
import sys
sys.path.insert(0, 'src')
from inference_erecap import load_model_and_pruners, MODEL_PATH, PRUNING_CKPT
import os
print('✓ MODEL_PATH:', MODEL_PATH)
print('✓ MODEL_PATH exists:', os.path.exists(MODEL_PATH))
print('✓ PRUNING_CKPT exists:', os.path.exists(PRUNING_CKPT))
print('✓ All checks passed!')
"
```

**Note:** If `MODEL_PATH` doesn't exist, edit `src/inference_erecap.py` and set `MODEL_PATH` to your model directory path.

#### Stage 1: Saliency Computation (Optional)

Only needed if `checkpoints/saliency.pt` doesn't exist:
```bash
bash scripts/run_stage1.sh 1000
```

**Note:** This stage uses the model specified in `src/stage1_saliency.py`. Make sure to set the correct model path there if using a different model.

#### Stage 2: Pruning Module Training (Required)

Only needed if `checkpoints/pruning_module.pt` doesn't exist:
```bash
bash scripts/run_stage2.sh 1e-4 2
```

Parameters:
- First argument: Learning rate (default: 1e-4)
- Second argument: Number of epochs (default: 2)

**Note:** This stage trains a model-specific pruning module. The trained `pruning_module.pt` is tied to the model's `hidden_size`. If you change models, you may need to retrain the pruning module if the new model has a different `hidden_size`.

#### Inference

**Single GPU Inference:**

1. **Prefill-only benchmark** (fast, ~5-10 minutes):
   ```bash
   bash scripts/run_inference.sh profile prefill
   ```

2. **End-to-end benchmark** (includes decode, ~15-30 minutes):
   ```bash
   bash scripts/run_inference.sh profile end2end
   ```

3. **Text generation test** (quick verification):
   ```bash
   bash scripts/run_inference.sh generate "Hello, E-RECAP!"
   ```

**Multi-GPU Inference** (for long sequences, 32K+ tokens):
```bash
# Multi-GPU profiling
bash scripts/run_inference_multigpu.sh profile

# Multi-GPU text generation
bash scripts/run_inference_multigpu.sh generate "Your prompt here"
```

**Run single configuration directly:**
```bash
cd src
python3 -u inference_erecap.py \
  --mode profile \
  --config keep07 \
  --benchmark_mode prefill \
  --lengths 1024 2048 4096
```

#### Results Location

- **Single GPU results**: `results/latency_results_keep*.json`
- **Multi-GPU results**: `results/latency_erecap_multigpu.json`
- **Baseline results**: `results/latency_baseline_keep*.json`

### Available Scripts

The `scripts/` directory contains helper scripts for common tasks:

#### Core Scripts

- **`run_inference.sh`**: Single-GPU inference and benchmarking
  - `bash scripts/run_inference.sh profile prefill` - Prefill-only benchmark
  - `bash scripts/run_inference.sh profile end2end` - End-to-end benchmark
  - `bash scripts/run_inference.sh generate "prompt"` - Text generation

- **`run_inference_multigpu.sh`**: Multi-GPU inference for long sequences
  - `bash scripts/run_inference_multigpu.sh profile` - Multi-GPU profiling
  - `bash scripts/run_inference_multigpu.sh generate "prompt"` - Multi-GPU generation

- **`run_stage1.sh`**: Generate saliency baseline (optional)
  - `bash scripts/run_stage1.sh [num_samples]` - Default: 1000 samples

- **`run_stage2.sh`**: Train pruning module (required if missing)
  - `bash scripts/run_stage2.sh [learning_rate] [epochs]` - Default: 1e-4, 2 epochs

#### Utility Scripts

- **`install.sh`**: Install Python dependencies and PyTorch
  - `bash scripts/install.sh`

- **`check_full_env.sh`**: Comprehensive environment check
  - `bash scripts/check_full_env.sh` - Verifies GPU, CUDA, Python, dependencies

- **`run_plot_latency.sh`**: Generate latency comparison plots
  - `bash scripts/run_plot_latency.sh [output_dir]` - Default: `results/fig`

- **`run_multigpu_test.sh`**: Test multi-GPU memory usage
  - `bash scripts/run_multigpu_test.sh` - Memory profiling for long sequences

#### Evaluation Scripts (Optional)

- **`run_lmeval.sh`**: Run lm-eval-harness evaluation
  - `bash scripts/run_lmeval.sh [baseline|erecap]` - Default: baseline

- **`run_lmeval_setup.sh`**: Setup lm-eval evaluation framework
  - `bash scripts/run_lmeval_setup.sh [task_config] [model_type] [output_dir]`

- **`run_longbench.sh`**: Run LongBench evaluation
  - `bash scripts/run_longbench.sh [task] [type] [num_samples]` - Default: hotpotqa, baseline, 30

- **`run_longbench_setup.sh`**: Setup LongBench evaluation
  - `bash scripts/run_longbench_setup.sh [task] [model] [pruning_module] [output]`

- **`run_ablation.sh`**: Run ablation study
  - `bash scripts/run_ablation.sh` - Generates ablation results

## E-RECAP Summary

E-RECAP implementation completed:
- ✅ Saliency baseline computation
- ✅ Token Pruner module training
- ✅ Single GPU inference (2.6-3× speedup)
- ✅ Multi-GPU inference (8-10× speedup)

## Key Features

- **Cost-Aware Pruning**: Remove redundant tokens during prefill to reduce computation
- **Layer-wise Pruning**: Progressive pruning across Transformer layers
- **Multi-GPU Support**: Automatic distributed inference for long sequences
- **Learnable Pruning Module**: Lightweight MLP for cost-aware token pruning

## Results

- **Single GPU** (NVIDIA RTX 5880 Ada, 48GB): E-RECAP 2.6-3.0× prefill speedup
- **Multi-GPU** (8× NVIDIA RTX 5880 Ada Generation, 48GB each): E-RECAP Up to 10× end-to-end speedup
- **Memory Savings**: Up to 34% GPU memory reduction
- **Performance**: E-RECAP Maintains comparable performance with 65% token pruning

## Model Configuration

E-RECAP supports any HuggingFace-compatible Transformer model. To use a different model:

1. **Place model files** in `checkpoints/<your-model-name>/`

2. **Update model path** in the following files:
   - `src/inference_erecap.py`: Set `MODEL_PATH = "checkpoints/<your-model-name>"`
   - `src/stage1_saliency.py`: Set `MODEL_PATH` (if running Stage 1)
   - `src/stage2_pruning.py`: Set `MODEL_NAME = "checkpoints/<your-model-name>"` (if running Stage 2)

3. **Train pruning module** (if switching to a model with different `hidden_size`):
   - The pruning module is model-specific and depends on the model's `hidden_size`
   - If your new model has the same `hidden_size`, you can reuse the existing `pruning_module.pt`
   - Otherwise, retrain by running Stage 2 with the new model

